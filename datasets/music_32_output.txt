=== microgpt (Pyodide) ===
dataset: Nottingham folk melodies (ABC) | docs: 538
vocab: 40 |  '()+,-/123468:=ABCDEFGH[\]^_abcdefgz|~
params: 4912
training 100 steps...
step 1/100 | loss 3.704781
step 2/100 | loss 3.616413
step 3/100 | loss 3.703196
step 4/100 | loss 3.499682
step 5/100 | loss 3.397884
step 6/100 | loss 3.372007
step 7/100 | loss 3.369694
step 8/100 | loss 3.206148
step 9/100 | loss 3.039022
step 10/100 | loss 3.167518
step 11/100 | loss 3.137081
step 12/100 | loss 3.033660
step 13/100 | loss 2.741173
step 14/100 | loss 2.799329
step 15/100 | loss 3.000226
step 16/100 | loss 2.696276
step 17/100 | loss 2.634709
step 18/100 | loss 2.825746
step 19/100 | loss 2.722420
step 20/100 | loss 2.713926
step 21/100 | loss 2.821886
step 22/100 | loss 2.603964
step 23/100 | loss 2.808915
step 24/100 | loss 2.836137
step 25/100 | loss 2.727627
step 26/100 | loss 2.788128
step 27/100 | loss 2.387500
step 28/100 | loss 2.702291
step 29/100 | loss 2.740087
step 30/100 | loss 3.669457
step 31/100 | loss 2.819557
step 32/100 | loss 2.414549
step 33/100 | loss 2.570742
step 34/100 | loss 2.096188
step 35/100 | loss 2.602441
step 36/100 | loss 2.625246
step 37/100 | loss 2.606209
step 38/100 | loss 2.615928
step 39/100 | loss 2.232425
step 40/100 | loss 2.762166
step 41/100 | loss 2.506140
step 42/100 | loss 2.456663
step 43/100 | loss 2.234418
step 44/100 | loss 2.666942
step 45/100 | loss 2.462684
step 46/100 | loss 2.421046
step 47/100 | loss 1.845632
step 48/100 | loss 2.317776
step 49/100 | loss 2.977541
step 50/100 | loss 2.557840
step 51/100 | loss 2.409706
step 52/100 | loss 2.467797
step 53/100 | loss 2.612742
step 54/100 | loss 3.115388
step 55/100 | loss 2.315201
step 56/100 | loss 3.315067
step 57/100 | loss 2.425074
step 58/100 | loss 2.611556
step 59/100 | loss 2.628646
step 60/100 | loss 2.226825
step 61/100 | loss 2.766436
step 62/100 | loss 2.408138
step 63/100 | loss 3.069826
step 64/100 | loss 2.132520
step 65/100 | loss 2.879070
step 66/100 | loss 2.393074
step 67/100 | loss 2.023001
step 68/100 | loss 2.706018
step 69/100 | loss 2.562737
step 70/100 | loss 2.459396
step 71/100 | loss 3.048921
step 72/100 | loss 2.504942
step 73/100 | loss 2.702827
step 74/100 | loss 2.302940
step 75/100 | loss 2.727647
step 76/100 | loss 2.288602
step 77/100 | loss 2.066926
step 78/100 | loss 2.517538
step 79/100 | loss 2.739530
step 80/100 | loss 2.763859
step 81/100 | loss 2.668162
step 82/100 | loss 2.442502
step 83/100 | loss 1.778480
step 84/100 | loss 2.455531
